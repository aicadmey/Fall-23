{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY1JGwuGCi9o"
      },
      "outputs": [],
      "source": [
        "#BASIC RNN PROJECT (IT ENCOUNTERS SOME ERRORS)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "#LOADING AND PREPROCESSING THE DATA\n",
        "text = open(\"shakespeare.txt\", \"r\").read()\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
        "ix2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[char] for char in text])\n",
        "\n",
        "#CREATING A SEQUENCE FOR TRAINING\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "#BUILD THE RNN MODEL\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(len(vocab), 256),\n",
        "    keras.layers.LSTM(1024, return_sequences=True),\n",
        "    keras.layers.Dense(len(vocab))\n",
        "])\n",
        "\n",
        "#COMPILE THE MODEL\n",
        "model.compile(optimizer=\"adam\", loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "#TRAINNIG THE MODEL\n",
        "model.fit(dataset, epochs=5)\n",
        "\n",
        "#GENERATE TEXT\n",
        "def generate_text(model, start_string):\n",
        "  num_generate = 1000\n",
        "  input_eval =[char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  text_generated = []\n",
        "  temperature = 0.7  #TEMPERATURE CAN BE ADJUSTED TO CONTROL THE RANDOMNESS OF TEXT GENERATION\n",
        "  model.rest_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "return(starting_string + \"\".join(text_generated))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#HERE IS ANOTHER BASIC RNN PROJECT(ERROR-FREE)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess the text data\n",
        "text = open('shakespeare.txt', 'r').read()\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[char] for char in text])\n",
        "\n",
        "# Create sequences for training\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Build the RNN model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(len(vocab), 256),\n",
        "    keras.layers.LSTM(1024, return_sequences=True),\n",
        "    keras.layers.Dense(len(vocab))\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "# Prepare the training data\n",
        "batch_size = 64\n",
        "steps_per_epoch = examples_per_epoch // batch_size\n",
        "BUFFER_SIZE = 10000\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "# Training the model\n",
        "model.fit(data, epochs=5)\n",
        "\n",
        "# Generate text as previously shown\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaCU-DRIJ0xS",
        "outputId": "5740484d-00e2-420b-a024-6d0afaf9ba0d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "14/14 [==============================] - 132s 9s/step - loss: 3.7523\n",
            "Epoch 2/5\n",
            "14/14 [==============================] - 123s 9s/step - loss: 3.1285\n",
            "Epoch 3/5\n",
            "14/14 [==============================] - 124s 9s/step - loss: 3.0424\n",
            "Epoch 4/5\n",
            "14/14 [==============================] - 123s 9s/step - loss: 2.9423\n",
            "Epoch 5/5\n",
            "14/14 [==============================] - 125s 9s/step - loss: 2.7978\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7919703fe860>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l1S8e8pBOq5C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}